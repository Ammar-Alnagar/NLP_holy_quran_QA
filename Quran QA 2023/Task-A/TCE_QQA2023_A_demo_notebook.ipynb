{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code by TCE team at Qur'an QA 2023 shared task A"
      ],
      "metadata": {
        "id": "Ombx8ieji4Ze"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftqSfDklodvR"
      },
      "source": [
        "# Installation\n",
        "\n",
        "I use [rclone](https://rclone.org/) to access my drive without asking for permission everytime.\n",
        "The code accesses a file called colab4 which has my drive access token, you may replicate this on your side or just ignore this altogether and download files manually.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu\n",
        "!nvidia-smi\n",
        "!free -g"
      ],
      "metadata": {
        "id": "AxOQ4vGN6mmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDxbUn4SqkZn"
      },
      "outputs": [],
      "source": [
        "!curl https://rclone.org/install.sh | bash 2> null 1>null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PADO5nbTdSM1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rclone"
      ],
      "metadata": {
        "id": "e4_yhMkDusbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "033ZSN1jEGNB"
      },
      "source": [
        "## Clone repo and prepare the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2k2ZNV3dVPz"
      },
      "outputs": [],
      "source": [
        "repo_url = f\"https://github.com/mohammed-elkomy/TCE-QQA2023-TASK-A\"\n",
        "!git clone $repo_url\n",
        "%cd TCE-QQA2023-TASK-A\n",
        "!pip install -r requirements.txt\n",
        "!pip install --no-deps python-terrier==0.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKhwQdUUEasK"
      },
      "source": [
        "### Download and create datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqlqPLD7dXvQ"
      },
      "outputs": [],
      "source": [
        "!git pull\n",
        "\n",
        "\n",
        "!python data_scripts/download_datasets.py > null 2> null\n",
        "!python data_scripts/generate/generate_tydi_qa_pretraining_pairs.py > null 2> null\n",
        "!python data_scripts/generate/merge_train_dev.py > null 2> null\n",
        "!python data_scripts/generate/generate_tafseer_data.py > null 2> null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!md5sum data/* | sort -k 2"
      ],
      "metadata": {
        "id": "oAuwBM1mflh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Download pretrained models\n",
        "Download those files from drive or huggingface\n",
        "\n",
        "**tydi-pairs ➡ trained only tydi-qa passage-question pairs**\n",
        "1. araelectra-base-discriminator-tydi-pairs\n",
        "2. bert-base-arabertv02-tydi-pairs\n",
        "3. bert-base-arabic-camelbert-ca-tydi-pairs\n",
        "\n",
        "**tydi-tafseer ➡ trained only tydi-qa passage-question pairs then tafseer pairs**\n",
        "4. bert-base-arabertv02-tydi-tafseer-pairs\n",
        "5. bert-base-arabic-camelbert-ca-tydi-tafseer-pairs\n",
        "6. araelectra-base-discriminator-tydi-tafseer-pairs\n",
        "\n",
        "**tafseer-pairs ➡ trained only tafseer pairs**\n",
        "7. bert-base-arabic-camelbert-ca-tafseer-pairs\n",
        "8. bert-base-arabertv02-tafseer-pairs\n",
        "9. araelectra-base-discriminator-tafseer-pairs"
      ],
      "metadata": {
        "id": "8TtqS0ZOLslx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure to make a copy for the nested DRhard repo\n",
        "!cp -r \"bi-bert-base-arabertv02-tafseer\" \"biencoder/DRhard/bi-bert-base-arabertv02-tafseer\""
      ],
      "metadata": {
        "id": "M-i55w5VLpWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to use colab for this notebook in order to see the interactive form of experiments.\n",
        "\n",
        "* We have different models to choose from the list below.\n",
        "* Set the number of models to train, we train 10 models to get average performance.\n",
        "\n",
        "* choose the experiment mode\n",
        "    1.  QQA23_TaskA_qrcd_v1.2 | QQA  ➡ normal training with official training data and validation with official validation data.  \n",
        "    2.  QQA23_TaskA_qrcd_v1.2_merged | QQA-merged ➡ combining training and validation for training and perform inference using hidden split (done for testing phase).\n",
        "    3. pretraining can be made either by \"tafseer\" or \"TYDI\" pairs.\n",
        "\n",
        "---\n",
        "\n",
        "**Once the training is made you will find a dump file saved!**\n",
        "\n",
        "something like: araelectra-base-discriminator-tafseer-pairs-fine-tuned-1e-06-5254-train.zip\n",
        "This is a araelectra-base-discriminator-tafseer-pairs fine-tuned model with:\n",
        "1. learning rate of 1e-06.\n",
        "2. A random starting seed of 5254.\n",
        "4. train.zip means training data is used\n",
        "\n",
        "This dump file contains models prediction for the given eval or test data."
      ],
      "metadata": {
        "id": "EXVXy1VBf3Qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Cross Encoder"
      ],
      "metadata": {
        "id": "3J7xOqxRKVif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cross encoder is a bert-based model that predicts a relevance score for a pair of sentences (late-interaction)\n",
        "\n",
        "* We have different models to choose from the list below.\n",
        "* Set the number of models to train, we train 10 models to get average performance.\n",
        "\n",
        "* choose the experiment mode\n",
        "    1.  QQA23_TaskA_qrcd_v1.2  ➡ normal training with official training data and validation with official validation data.  \n",
        "    2.  QQA23_TaskA_qrcd_v1.2_merged ➡ combining training and validation for training and perform inference using hidden split (done for testing phase).\n",
        "    3. tafseer  ➡ For tafseer pratraining data pairs\n",
        "    4. pre-train ➡ For tydi-qa pratraining data pairs\n",
        "\n",
        "[Check this for more details](https://www.sbert.net/examples/applications/cross-encoder/README.html)"
      ],
      "metadata": {
        "id": "jUqgZaJYhJbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from random import choice\n",
        "import glob\n",
        "\n",
        "model_name = \"aubmindlab/araelectra-base-discriminator\"  # @param [\"araelectra-base-discriminator-tydi-tafseer-pairs\", \"bert-base-arabic-camelbert-ca-tydi-tafseer-pairs\", \"bert-base-arabertv02-tydi-tafseer-pairs\", \"====\", \"araelectra-base-discriminator-tydi-pairs\", \"bert-base-arabertv02-tydi-pairs\", \"bert-base-arabic-camelbert-ca-tydi-pairs\", \"===\", \"bert-base-arabic-camelbert-ca-tafseer-pairs\", \"bert-base-arabertv02-tafseer-pairs\", \"araelectra-base-discriminator-tafseer-pairs\", \"====\", \"aubmindlab/bert-base-arabertv02\", \"CAMeL-Lab/bert-base-arabic-camelbert-ca\", \"aubmindlab/araelectra-base-discriminator\"]\n",
        "\n",
        "num_models = 1 # @param {type:\"integer\"}\n",
        "\n",
        "experiment_mode = \"tafseer\"  # @param [\"QQA23_TaskA_qrcd_v1.2\", \"QQA23_TaskA_qrcd_v1.2_merged\",\"all_dev\",\"pre-train\",\"tafseer\"]\n",
        "\n",
        "lr = \"1e-6\"  # @param [\"2e-5\",\"1e-5\",\"5e-6\",\"2e-6\",\"1e-6\"]\n",
        "\n",
        "\n",
        "for idx in range(num_models):\n",
        "    out_file = f\"{idx}-out.txt\"\n",
        "    err_file = f\"{idx}-err.txt\"\n",
        "    doc_file=\"data/QQA23_TaskA_QPC_v1.1.tsv\"\n",
        "\n",
        "    if experiment_mode == \"QQA23_TaskA_qrcd_v1.2_merged\":\n",
        "        train_qrel_file = \"data/QQA23_TaskA_qrels_merged.gold\"\n",
        "        train_query_file = \"data/QQA23_TaskA_merged.tsv\"\n",
        "    elif experiment_mode == \"QQA23_TaskA_qrcd_v1.2\":\n",
        "        train_qrel_file = \"data/QQA23_TaskA_qrels_train.gold\"\n",
        "        train_query_file = \"data/QQA23_TaskA_train.tsv\"\n",
        "    elif experiment_mode == \"all_dev\":\n",
        "        train_qrel_file = \"data/QQA23_TaskA_qrels_dev.gold\"\n",
        "        train_query_file = \"data/QQA23_TaskA_dev.tsv\"\n",
        "\n",
        "    validation_qrel_file = \"data/QQA23_TaskA_qrels_dev.gold\"\n",
        "    validation_query_file = \"data/QQA23_TaskA_dev.tsv\"\n",
        "\n",
        "    test_qrel_file = None\n",
        "    test_query_file = \"data/QQA23_TaskA_test.tsv\"\n",
        "    num_train_epochs = 10\n",
        "    pre_train = False\n",
        "    do_predict = True\n",
        "    do_eval= True\n",
        "    if experiment_mode == \"pre-train\":\n",
        "        doc_file=\"data/TYDI_QA_DOC.tsv\"\n",
        "        train_qrel_file = \"data/TYDI_QA_qrels_train.gold\"\n",
        "        train_query_file = \"data/TYDI_QA_train.tsv\"\n",
        "        validation_qrel_file = \"data/TYDI_QA_qrels_dev.gold\"\n",
        "        validation_query_file = \"data/TYDI_QA_dev.tsv\"\n",
        "        test_qrel_file = None\n",
        "        test_query_file = None\n",
        "        pre_train = True\n",
        "        do_predict = False\n",
        "        num_train_epochs = 2\n",
        "\n",
        "    if experiment_mode == \"tafseer\":\n",
        "        doc_file=\"data/tafseer_docs.tsv\"\n",
        "        train_qrel_file = \"data/tafseer-qrel.tsv\"\n",
        "        train_query_file = \"data/tafseer-query.tsv\"\n",
        "        validation_qrel_file = None\n",
        "        validation_query_file = None\n",
        "        test_qrel_file = None\n",
        "        test_query_file = None\n",
        "        pre_train = True\n",
        "        do_eval= False\n",
        "        do_predict = False\n",
        "        num_train_epochs = 5\n",
        "\n",
        "\n",
        "    output_folder = os.path.split(model_name)[-1] + f\"-fine-tuned-{float(lr)}\"\n",
        "\n",
        "    batch_size = 8 if \"large\" in model_name else 16\n",
        "\n",
        "\n",
        "    !git pull\n",
        "    !rm -r $output_folder\n",
        "\n",
        "    !python \"cross_encoder/trainer.py\" \\\n",
        "            --model_name_or_path  $model_name \\\n",
        "            --do_train True \\\n",
        "            --do_eval $do_eval \\\n",
        "            --do_predict $do_predict \\\n",
        "            --save_last_checkpoint_to_drive $pre_train \\\n",
        "            --train_qrel_file $train_qrel_file \\\n",
        "            --train_query_file  $train_query_file \\\n",
        "            --validation_qrel_file  $validation_qrel_file \\\n",
        "            --validation_query_file $validation_query_file \\\n",
        "            --test_qrel_file $test_qrel_file  \\\n",
        "            --test_query_file  $test_query_file \\\n",
        "            --doc_file $doc_file \\\n",
        "            --learning_rate $lr \\\n",
        "            --num_train_epochs $num_train_epochs \\\n",
        "            --max_seq_length 512 \\\n",
        "            --output_dir $output_folder \\\n",
        "            --per_device_eval_batch_size $batch_size \\\n",
        "            --per_device_train_batch_size $batch_size \\\n",
        "            --save_steps 2 \\\n",
        "            --overwrite_output_dir"
      ],
      "metadata": {
        "id": "PsNLwCpQssir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dual-encoder"
      ],
      "metadata": {
        "id": "iWCDuq2NwDEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dual-encoder is a bert-based model that predicts a relevance score for a pair of sentences represented individually (representational-based).\n",
        "The following cells trains ➡ infers ➡ mines hard negatives ➡ trains again.\n",
        "\n",
        "* We have different models to choose from the list below.\n",
        "* Set the number of models to train, we train 10 models to get average performance.\n",
        "\n",
        "* choose the experiment mode\n",
        "    1. QQA  ➡ normal training with official training data and validation with official validation data.  \n",
        "    2. QQA-merged ➡ combining training and validation for training and perform inference using hidden split (done for testing phase).\n",
        "    3. TYDI ➡ For tydi-qa pratraining data pairs\n",
        "\n",
        "[Check DRhard repo for more details](https://github.com/jingtaozhan/DRhard)"
      ],
      "metadata": {
        "id": "ycwRw1e-hp6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull\n",
        "import os\n",
        "from random import choice\n",
        "import glob\n",
        "\n",
        "if \"biencoder\" not in os.getcwd():\n",
        "    repo_root = os.path.join(os.getcwd(),\"biencoder\",\"DRhard\",)\n",
        "    %cd $repo_root\n",
        "\n",
        "model_name = \"bi-bert-base-arabertv02-tafseer\"  # @param [\"bi-bert-base-arabertv02-tafseer\",\"intfloat/multilingual-e5-base\", \"aubmindlab/bert-base-arabertv02\", \"CAMeL-Lab/bert-base-arabic-camelbert-ca\", \"aubmindlab/araelectra-base-discriminator\" ]\n",
        "\n",
        "num_models = 1 # @param {type:\"integer\"}\n",
        "\n",
        "experiment_mode = \"QQA\"  # @param [\"QQA\",\"QQA-merged\",\"TYDI\"]\n",
        "\n",
        "lr = \"5e-5\"  # @param [\"1e-5\",\"5e-5\",\"5e-6\",\"2e-6\",\"1e-6\",\"1e-4\"]\n",
        "\n",
        "!python preprocess.py --data_type $experiment_mode --threads 2 --model_name_or_path $model_name\n",
        "# max_query_length, max_doc_length are printed in preprocess script\n",
        "max_doc_length = 335\n",
        "max_query_length = 64\n",
        "for idx in range(num_models):\n",
        "    out_file = f\"{idx}-out.txt\"\n",
        "    err_file = f\"{idx}-err.txt\"\n",
        "    doc_file=\"data/QQA23_TaskA_QPC_v1.1.tsv\"\n",
        "\n",
        "    num_train_epochs = 100\n",
        "    pre_train = False\n",
        "    do_predict = True\n",
        "    if experiment_mode == \"TYDI\":\n",
        "        pre_train = True\n",
        "        do_predict = False\n",
        "        num_train_epochs = 2\n",
        "\n",
        "    output_folder = os.path.split(model_name)[-1] + f\"-fine-tuned-{float(lr)}\"\n",
        "\n",
        "    batch_size = 8 if \"large\" in model_name else 16\n",
        "\n",
        "    !python star/train.py --do_train \\\n",
        "        --max_query_length $max_query_length \\\n",
        "        --max_doc_length $max_doc_length \\\n",
        "        --preprocess_dir ./data/QQA/preprocess \\\n",
        "        --init_path  $model_name \\\n",
        "        --output_dir ./data/QQA/star_train/models \\\n",
        "        --logging_dir ./data/QQA/star_train/log \\\n",
        "        --optimizer_str adamw \\\n",
        "        --learning_rate $lr \\\n",
        "        --save_every_epochs 50 \\\n",
        "        --overwrite_output_dir --num_train_epochs $num_train_epochs \\\n",
        "        --per_device_train_batch_size $batch_size\n",
        "\n",
        "    # !rm -r /content/TCE-QQA2023-TASK-A/biencoder/DRhard/data/QQA/star_train/models\n"
      ],
      "metadata": {
        "id": "2AvfT-eh7n-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull\n",
        "\n",
        "# ./data/QQA/star_train/models has the trained model\n",
        "\n",
        "if \"biencoder\" not in os.getcwd():\n",
        "    repo_root = os.path.join(os.getcwd(),\"biencoder\",\"DRhard\",)\n",
        "    %cd $repo_root\n",
        "\n",
        "!mkdir -p './data/QQA/trained_models/star'\n",
        "!cp -r \"data/QQA/star_train/models/checkpoint-1000/.\"  './data/QQA/trained_models/star'\n",
        "\n",
        "# './data/QQA/trained_models/star' this is used by inference.py\n",
        "!rm -r \"data/QQA/evaluate/star/\"\n",
        "!python star/inference.py --data_type QQA \\\n",
        "    --max_query_length $max_query_length \\\n",
        "    --max_doc_length $max_doc_length \\\n",
        "    --mode dev \\\n",
        "    --eval_batch_size 256 \\\n",
        "    --do_full_retrieval \\\n",
        "    --topk 1000 \\\n",
        "    --no_tpu --faiss_gpus 0\n",
        "\n",
        "!python ./cvt_back.py \\\n",
        "    --input_dir ./data/QQA/evaluate/star/ \\\n",
        "    --preprocess_dir ./data/QQA/preprocess \\\n",
        "    --output_dir ./data/QQA/official_runs/star \\\n",
        "    --mode dev --dataset QQA\n",
        "\n",
        "%cd \"/content/TCE-QQA2023-TASK-A\"\n",
        "!python \"metrics/Custom_TaskA_eval.py\" \\\n",
        "    --run=\"/content/TCE-QQA2023-TASK-A/biencoder/DRhard/data/QQA/official_runs/star/dev.rank.tsv\" \\\n",
        "    --qrels=\"data/QQA23_TaskA_qrels_dev.gold\""
      ],
      "metadata": {
        "id": "l3DjPUC56kO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull\n",
        "if \"biencoder\" not in os.getcwd():\n",
        "    repo_root = os.path.join(os.getcwd(),\"biencoder\",\"DRhard\",)\n",
        "    %cd $repo_root\n",
        "\n",
        "!rm -r \"data/QQA/warmup\"\n",
        "!cp -r \"data/QQA/trained_models/star\" \"data/QQA/warmup\"\n",
        "\n",
        "!python star/prepare_hardneg.py \\\n",
        "    --data_type QQA \\\n",
        "    --max_query_length $max_query_length \\\n",
        "    --max_doc_length $max_doc_length  \\\n",
        "    --mode train \\\n",
        "    --topk 200 \\\n",
        "    --eval_batch_size 64 \\\n",
        "    --max_positives 200 \\\n",
        "    --output_inference_dir \"star-hard-prepare\""
      ],
      "metadata": {
        "id": "JqQPN61cX11o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"biencoder\" not in os.getcwd():\n",
        "    repo_root = os.path.join(os.getcwd(),\"biencoder\",\"DRhard\",)\n",
        "    %cd $repo_root\n",
        "\n",
        "# warmup model is the trained model from random negatives (copied at the last cell)\n",
        "!python ./star/train.py --do_train \\\n",
        "    --max_query_length $max_query_length \\\n",
        "    --max_doc_length $max_doc_length \\\n",
        "    --preprocess_dir ./data/QQA/preprocess \\\n",
        "    --hardneg_path  ./data/QQA/star-hard-prepare/hard.json \\\n",
        "    --init_path ./data/QQA/warmup \\\n",
        "    --output_dir ./data/QQA/star_train_hard/models \\\n",
        "    --logging_dir ./data/QQA/star_train_hard/log \\\n",
        "    --optimizer_str adamw \\\n",
        "    --learning_rate $lr \\\n",
        "    --save_every_epochs 25 \\\n",
        "    --per_device_train_batch_size $batch_size \\\n",
        "    --overwrite_output_dir --num_train_epochs 100"
      ],
      "metadata": {
        "id": "PMFgxvV9gpFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ./data/QQA/star_train_hard/models has the trained model\n",
        "!git pull\n",
        "\n",
        "if \"biencoder\" not in os.getcwd():\n",
        "    repo_root = os.path.join(os.getcwd(),\"biencoder\",\"DRhard\",)\n",
        "    %cd $repo_root\n",
        "\n",
        "!mkdir -p './data/QQA/trained_models/star'\n",
        "!cp -r \"data/QQA/star_train_hard/models/checkpoint-1000/.\"  './data/QQA/trained_models/star'\n",
        "\n",
        "!rm -r \"data/QQA/evaluate/star/\"\n",
        "!python star/inference.py --data_type QQA \\\n",
        "    --max_query_length $max_query_length \\\n",
        "    --max_doc_length $max_doc_length \\\n",
        "    --mode dev \\\n",
        "    --eval_batch_size 256 \\\n",
        "    --do_full_retrieval \\\n",
        "    --topk 1000 \\\n",
        "    --no_tpu --faiss_gpus 0\n",
        "\n",
        "!python ./cvt_back.py \\\n",
        "    --input_dir ./data/QQA/evaluate/star/ \\\n",
        "    --preprocess_dir ./data/QQA/preprocess \\\n",
        "    --output_dir ./data/QQA/official_runs/star-hard \\\n",
        "    --mode dev --dataset QQA\n",
        "\n",
        "%cd \"/content/TCE-QQA2023-TASK-A\"\n",
        "!python \"metrics/Custom_TaskA_eval.py\" \\\n",
        "    --run=\"/content/TCE-QQA2023-TASK-A/biencoder/DRhard/data/QQA/official_runs/star-hard/dev.rank.tsv\" \\\n",
        "    --qrels=\"data/QQA23_TaskA_qrels_dev.gold\""
      ],
      "metadata": {
        "id": "IAE7ClawbRFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new warm up model from the hard negatives trained model\n",
        "\n",
        "if \"biencoder\" not in os.getcwd():\n",
        "    repo_root = os.path.join(os.getcwd(),\"biencoder\",\"DRhard\",)\n",
        "    %cd $repo_root\n",
        "\n",
        "!rm -r \"data/QQA/warmup\"\n",
        "!cp -r \"data/QQA/trained_models/star\" \"data/QQA/warmup\"\n",
        "\n",
        "!python star/prepare_hardneg.py \\\n",
        "    --data_type QQA \\\n",
        "    --max_query_length $max_query_length \\\n",
        "    --max_doc_length $max_doc_length  \\\n",
        "    --mode train \\\n",
        "    --topk 200 \\\n",
        "    --eval_batch_size 64 \\\n",
        "    --max_positives 200 \\\n",
        "    --output_inference_dir \"star-hard-prepare2\""
      ],
      "metadata": {
        "id": "31SjQ_FGZT39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"biencoder\" not in os.getcwd():\n",
        "    repo_root = os.path.join(os.getcwd(),\"biencoder\",\"DRhard\",)\n",
        "    %cd $repo_root\n",
        "\n",
        "\n",
        "!python ./star/train.py --do_train \\\n",
        "    --max_query_length $max_query_length \\\n",
        "    --max_doc_length $max_doc_length \\\n",
        "    --preprocess_dir ./data/QQA/preprocess \\\n",
        "    --hardneg_path  ./data/QQA/star-hard-prepare2/hard.json \\\n",
        "    --init_path ./data/QQA/warmup \\\n",
        "    --output_dir ./data/QQA/star_train_hard2/models \\\n",
        "    --logging_dir ./data/QQA/star_train_hard2/log \\\n",
        "    --optimizer_str adamw \\\n",
        "    --learning_rate $lr \\\n",
        "    --save_every_epochs 25 \\\n",
        "    --per_device_train_batch_size $batch_size \\\n",
        "    --overwrite_output_dir --num_train_epochs 100"
      ],
      "metadata": {
        "id": "NGysn31grG2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ./data/QQA/star_train_hard/models has the trained model\n",
        "!git pull\n",
        "\n",
        "if \"biencoder\" not in os.getcwd():\n",
        "    repo_root = os.path.join(os.getcwd(),\"biencoder\",\"DRhard\",)\n",
        "    %cd $repo_root\n",
        "\n",
        "!mkdir -p './data/QQA/trained_models/star'\n",
        "!cp -r \"data/QQA/star_train_hard2/models/checkpoint-1000/.\"  './data/QQA/trained_models/star'\n",
        "\n",
        "!rm -r \"data/QQA/evaluate/star/\"\n",
        "!python star/inference.py --data_type QQA \\\n",
        "    --max_query_length $max_query_length \\\n",
        "    --max_doc_length $max_doc_length \\\n",
        "    --mode dev \\\n",
        "    --eval_batch_size 256 \\\n",
        "    --do_full_retrieval \\\n",
        "    --topk 1000 \\\n",
        "    --no_tpu --faiss_gpus 0\n",
        "\n",
        "!python ./cvt_back.py \\\n",
        "    --input_dir ./data/QQA/evaluate/star/ \\\n",
        "    --preprocess_dir ./data/QQA/preprocess \\\n",
        "    --output_dir ./data/QQA/official_runs/star-hard2 \\\n",
        "    --mode dev --dataset QQA\n",
        "\n",
        "%cd \"/content/TCE-QQA2023-TASK-A\"\n",
        "!python \"metrics/Custom_TaskA_eval.py\" \\\n",
        "    --run=\"/content/TCE-QQA2023-TASK-A/biencoder/DRhard/data/QQA/official_runs/star-hard2/dev.rank.tsv\" \\\n",
        "    --qrels=\"data/QQA23_TaskA_qrels_dev.gold\""
      ],
      "metadata": {
        "id": "y0ayyz1wTazp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sbert"
      ],
      "metadata": {
        "id": "e8ocqV7GNLU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A biencoder is a bert-based model that predicts a relevance score for a pair of sentences represented individually (representational-based).\n",
        "The following cell only trains using random negatives.\n",
        "\n",
        "* We have different models to choose from the list below.\n",
        "* Set the number of models to train, we train 10 models to get average performance.\n",
        "\n",
        "* choose the experiment mode\n",
        "    1.  QQA23_TaskA_qrcd_v1.2  ➡ normal training with official training data and validation with official validation data.  \n",
        "    2.  QQA23_TaskA_qrcd_v1.2_merged ➡ combining training and validation for training and perform inference using hidden split (done for testing phase).\n",
        "    3. tafseer  ➡ For tafseer pratraining data pairs\n",
        "    4. pre-train ➡ For tydi-qa pratraining data pairs\n",
        "\n",
        "[Check this for more details](https://www.sbert.net/examples/applications/cross-encoder/README.html)"
      ],
      "metadata": {
        "id": "RcatG94ribII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from random import choice\n",
        "import glob\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv02\"  # @param [\"bi-bert-base-arabertv02-tafseer\",\"intfloat/multilingual-e5-base\", \"aubmindlab/bert-base-arabertv02\", \"CAMeL-Lab/bert-base-arabic-camelbert-ca\", \"aubmindlab/araelectra-base-discriminator\" ]\n",
        "\n",
        "num_models = 1 # @param {type:\"integer\"}\n",
        "\n",
        "experiment_mode = \"tafseer\"  # @param [\"QQA23_TaskA_qrcd_v1.2\", \"QQA23_TaskA_qrcd_v1.2_merged\",\"all_dev\",\"pre-train\",\"tafseer\"]\n",
        "\n",
        "lr = \"1e-6\"  # @param [\"2e-5\",\"1e-5\",\"5e-6\",\"2e-6\",\"1e-6\"]\n",
        "\n",
        "\n",
        "for idx in range(num_models):\n",
        "    out_file = f\"{idx}-out.txt\"\n",
        "    err_file = f\"{idx}-err.txt\"\n",
        "    doc_file=\"data/QQA23_TaskA_QPC_v1.1.tsv\"\n",
        "\n",
        "    if experiment_mode == \"QQA23_TaskA_qrcd_v1.2_merged\":\n",
        "        train_qrel_file = \"data/QQA23_TaskA_qrels_merged.gold\"\n",
        "        train_query_file = \"data/QQA23_TaskA_merged.tsv\"\n",
        "    elif experiment_mode == \"QQA23_TaskA_qrcd_v1.2\":\n",
        "        train_qrel_file = \"data/QQA23_TaskA_qrels_train.gold\"\n",
        "        train_query_file = \"data/QQA23_TaskA_train.tsv\"\n",
        "    elif experiment_mode == \"all_dev\":\n",
        "        train_qrel_file = \"data/QQA23_TaskA_qrels_dev.gold\"\n",
        "        train_query_file = \"data/QQA23_TaskA_dev.tsv\"\n",
        "\n",
        "    validation_qrel_file = \"data/QQA23_TaskA_qrels_dev.gold\"\n",
        "    validation_query_file = \"data/QQA23_TaskA_dev.tsv\"\n",
        "\n",
        "    test_qrel_file = \"data/QQA23_TaskA_qrels_dev.gold\"\n",
        "    test_query_file = \"data/QQA23_TaskA_dev.tsv\"\n",
        "    num_train_epochs = 10\n",
        "    pre_train = False\n",
        "    do_predict = True\n",
        "    do_eval= True\n",
        "    if experiment_mode == \"pre-train\":\n",
        "        doc_file=\"data/TYDI_QA_DOC.tsv\"\n",
        "        train_qrel_file = \"data/TYDI_QA_qrels_train.gold\"\n",
        "        train_query_file = \"data/TYDI_QA_train.tsv\"\n",
        "        validation_qrel_file = \"data/TYDI_QA_qrels_dev.gold\"\n",
        "        validation_query_file = \"data/TYDI_QA_dev.tsv\"\n",
        "        test_qrel_file = None\n",
        "        test_query_file = None\n",
        "        pre_train = True\n",
        "        do_predict = False\n",
        "        num_train_epochs = 2\n",
        "\n",
        "    if experiment_mode == \"tafseer\":\n",
        "        doc_file=\"data/tafseer_docs.tsv\"\n",
        "        train_qrel_file = \"data/tafseer-qrel.tsv\"\n",
        "        train_query_file = \"data/tafseer-query.tsv\"\n",
        "        validation_qrel_file = None\n",
        "        validation_query_file = None\n",
        "        test_qrel_file = None\n",
        "        test_query_file = None\n",
        "        pre_train = True\n",
        "        do_eval= False\n",
        "        do_predict = False\n",
        "        num_train_epochs = 5\n",
        "\n",
        "\n",
        "    output_folder = os.path.split(model_name)[-1] + f\"-fine-tuned-{float(lr)}\"\n",
        "\n",
        "    batch_size = 8 if \"large\" in model_name else 16\n",
        "\n",
        "\n",
        "    !git pull\n",
        "    !rm -r $output_folder\n",
        "\n",
        "    !python \"sbert/sbert_trainer.py\" \\\n",
        "            --model_name_or_path  $model_name \\\n",
        "            --do_train True \\\n",
        "            --do_eval $do_eval \\\n",
        "            --do_predict $do_predict \\\n",
        "            --save_last_checkpoint_to_drive $pre_train \\\n",
        "            --train_qrel_file $train_qrel_file \\\n",
        "            --train_query_file  $train_query_file \\\n",
        "            --validation_qrel_file  $validation_qrel_file \\\n",
        "            --validation_query_file $validation_query_file \\\n",
        "            --test_qrel_file $test_qrel_file  \\\n",
        "            --test_query_file  $test_query_file \\\n",
        "            --doc_file $doc_file \\\n",
        "            --learning_rate $lr \\\n",
        "            --num_train_epochs $num_train_epochs \\\n",
        "            --max_seq_length 512 \\\n",
        "            --output_dir $output_folder \\\n",
        "            --per_device_eval_batch_size $batch_size \\\n",
        "            --per_device_train_batch_size $batch_size \\\n",
        "            --save_steps 2 \\\n",
        "            --overwrite_output_dir"
      ],
      "metadata": {
        "id": "K4QQv5KtNMrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis and ensemble\n",
        "\n",
        "**Once the training is made you will find a dump file saved!**\n",
        "\n",
        "**Once the training is made you will find a dump file saved!**\n",
        "\n",
        "something like: araelectra-base-discriminator-tafseer-pairs-fine-tuned-1e-06-5254-train.zip\n",
        "This is an araelectra-base-discriminator-tafseer-pairs fine-tuned model with:\n",
        "1. learning rate of 1e-06.\n",
        "2. A random starting seed of 5254.\n",
        "4. train.zip means training data is used\n",
        "\n",
        "This dump file contains models prediction for the given eval or test data.\n",
        "\n",
        "You can look at the **analysis** directory of the repo for more details.\n",
        "You can group dump files into folders:\n",
        "1. run **performance_analysis.py** script to process and get results for single models and ensemble models\n",
        "   - **retrieval_ensemble.py** is consumed by **performance_analysis.py** to implement the ensemble logic\n"
      ],
      "metadata": {
        "id": "WXI1IZUJ-SKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python analysis/performance_analysis.py"
      ],
      "metadata": {
        "id": "wwXLK2Kx-Rv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "collapsed_sections": [
        "3J7xOqxRKVif",
        "iWCDuq2NwDEt",
        "e8ocqV7GNLU4"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}