{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code by TCE team at Qur'an QA 2023 shared task B"
      ],
      "metadata": {
        "id": "INL6miU2jAX6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftqSfDklodvR"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use [rclone](https://rclone.org/) to access my drive without asking for permission everytime.\n",
        "The code accesses a file called colab4 which has my drive access token, you may replicate this on your side or just ignore this altogether and download files manually.  "
      ],
      "metadata": {
        "id": "tZ5KGIn-qIgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu\n",
        "!nvidia-smi\n",
        "!free -g"
      ],
      "metadata": {
        "id": "AxOQ4vGN6mmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDxbUn4SqkZn"
      },
      "outputs": [],
      "source": [
        "!curl https://rclone.org/install.sh | bash 2> null 1>null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PADO5nbTdSM1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rclone"
      ],
      "metadata": {
        "id": "e4_yhMkDusbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "033ZSN1jEGNB"
      },
      "source": [
        "## Clone repo and prepare the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2k2ZNV3dVPz"
      },
      "outputs": [],
      "source": [
        "repo_url = f\"https://github.com/mohammed-elkomy/TCE-QQA2023-TASK-B\"\n",
        "!git clone $repo_url\n",
        "%cd TCE-QQA2023-TASK-B\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKhwQdUUEasK"
      },
      "source": [
        "### Download and create datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqlqPLD7dXvQ"
      },
      "outputs": [],
      "source": [
        "!git pull\n",
        "\n",
        "!python data_scripts/download_datasets.py\n",
        "!python data_scripts/generate/generate_faithful_splits.py\n",
        "!python data_scripts/generate/qrcd_merge_train_dev.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!md5sum data/* | sort -k 2"
      ],
      "metadata": {
        "id": "oAuwBM1mflh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Download pretrained models\n",
        "Download those files from drive or huggingface\n",
        "1. araelectra-base-discriminator-2703-arabic_tydiqa\n",
        "2. bert-base-arabertv02-440-arabic_tydiqa\n",
        "3. bert-base-arabic-camelbert-ca-78123-arabic_tydiqa"
      ],
      "metadata": {
        "id": "8TtqS0ZOLslx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "3J7xOqxRKVif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to use colab for this notebook in order to see the interactive form of experiments.\n",
        "\n",
        "* We have 6 different models to choose from listed below.\n",
        "* Set the number of models to train, we train 20 models to get average performance.\n",
        "* choose the experiment mode\n",
        "\n",
        "    1.  QQA23_TaskB_qrcd_v1.2 ➡ normal training with official training data and validation with official validation data.  \n",
        "    2. my-faithful-processed ➡ combines training and validation data to create faithful splits to address leakage (check the paper for more).\n",
        "    3.   QQA23_TaskB_qrcd_v1.2_merged ➡ combining training and validation for training and perform inference using hidden split (done for testing phase).\n",
        "\n",
        "* loss type: is the way of training\n",
        "    1. first ➡ only use the first answer for multi-answer questions.\n",
        "    2. MAL ➡ the model trained to jointly optimize on **all** answers for multi-answer samples\n",
        "\n",
        "---\n",
        "\n",
        "**Once the training is made you will find a dump file saved!**\n",
        "\n",
        "something like: bert-base-arabertv02-fine-tuned-2e-05-first-827-QQA23_TaskB_qrcd_v1.2_train.zip\n",
        "This is a bert-base-arabertv02 fine-tuned model with:\n",
        "1. learning rate of 2e-05.\n",
        "2. first learning method.\n",
        "3. A random starting seed of 827.\n",
        "4. QQA23_TaskB_qrcd_v1.2_train training data is used\n",
        "\n",
        "This dump file contains all summary results and model predictions for each sample.\n",
        "You can look at the **analysis** directory of the repo for more details.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-j0Q-iyyrJeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from random import choice\n",
        "import glob\n",
        "\n",
        "model_name = \"aubmindlab/araelectra-base-discriminator\"  # @param [\"bert-base-arabic-camelbert-ca-78123-arabic_tydiqa\", \"araelectra-base-discriminator-2703-arabic_tydiqa\", \"bert-base-arabertv02-440-arabic_tydiqa\",\"------\", \"aubmindlab/bert-base-arabertv02\", \"CAMeL-Lab/bert-base-arabic-camelbert-ca\", \"aubmindlab/araelectra-base-discriminator\" ]\n",
        "\n",
        "num_models = 19 # @param {type:\"integer\"}\n",
        "\n",
        "experiment_mode = \"my-faithful-processed\"  # @param [\"QQA23_TaskB_qrcd_v1.2\", \"QQA23_TaskB_qrcd_v1.2_merged\", \"my-faithful-processed\"]\n",
        "loss_type = \"MAL\"  # @param ['first', 'MAL']\n",
        "\n",
        "lr = \"2e-5\"  # @param [\"2e-5\",\"1e-5\",\"5e-6\",\"1e-6\"]\n",
        "\n",
        "pairwise_decoder = True # insensitive parameter (linear vs all-pairs decoding)\n",
        "\n",
        "for idx in range(num_models):\n",
        "    out_file = f\"{idx}-out.txt\"\n",
        "    err_file = f\"{idx}-err.txt\"\n",
        "    if experiment_mode == \"my-faithful-processed\":\n",
        "        train_file = choice(glob.glob(\"data/my-faithful-processed*_train*\"))\n",
        "        train_file = os.path.split(train_file)[-1]\n",
        "        validation_file = train_file.replace(\"_train\",\"_dev\")\n",
        "    elif experiment_mode == \"QQA23_TaskB_qrcd_v1.2_merged\":\n",
        "        train_file = \"QQA23_TaskB_qrcd_v1.2_merged_preprocessed.jsonl\"\n",
        "        validation_file = None\n",
        "    elif experiment_mode == \"QQA23_TaskB_qrcd_v1.2\":\n",
        "        train_file = experiment_mode+ \"_train_preprocessed.jsonl\"\n",
        "        validation_file = experiment_mode+ \"_dev_preprocessed.jsonl\"\n",
        "    else:\n",
        "        train_file = experiment_mode+ \"_train.jsonl\"\n",
        "        validation_file = experiment_mode+ \"_dev.jsonl\"\n",
        "\n",
        "    output_folder = os.path.split(model_name)[-1] + f\"-fine-tuned-{float(lr)}\" + \"-\" +loss_type\n",
        "    if not pairwise_decoder:\n",
        "        output_folder += \"-linear\"\n",
        "\n",
        "    batch_size = 8 if \"large\" in model_name else 16\n",
        "\n",
        "    print(train_file)\n",
        "    print(validation_file)\n",
        "    train_file = os.path.join(\"../../data\",train_file)\n",
        "\n",
        "    !git pull\n",
        "    !rm -r $output_folder\n",
        "\n",
        "    if validation_file:\n",
        "        validation_file = os.path.join(\"../../data\",validation_file)\n",
        "        !python \"runners/run_qa.py\" \\\n",
        "            --model_name_or_path  $model_name \\\n",
        "            --dataset \"data_scripts/loader_scripts/qrcd_v1_2_dataset_loader.py\" \\\n",
        "            --do_train \\\n",
        "            --do_eval \\\n",
        "            --do_predict \\\n",
        "            --per_device_train_batch_size $batch_size \\\n",
        "            --learning_rate $lr \\\n",
        "            --num_train_epochs 50 \\\n",
        "            --max_seq_length 384 \\\n",
        "            --doc_stride 128 \\\n",
        "            --max_answer_length 35 \\\n",
        "            --output_dir $output_folder \\\n",
        "            --overwrite_output_dir  \\\n",
        "            --overwrite_cache \\\n",
        "            --train_file $train_file \\\n",
        "            --validation_file $validation_file \\\n",
        "            --test_file  $validation_file \\\n",
        "            --save_total_limit 2 \\\n",
        "            --save_strategy \"epoch\" \\\n",
        "            --eval_steps 3 \\\n",
        "            --eval_metric \"metrics/QQA23_metric.py\" \\\n",
        "            --evaluation_strategy \"epoch\" \\\n",
        "            --metric_for_best_model 'eval_pAP@10' \\\n",
        "            --load_best_model_at_end  True \\\n",
        "            --greater_is_better True \\\n",
        "            --pairwise_decoder $pairwise_decoder \\\n",
        "            --loss_type  $loss_type  >$out_file 2> $err_file\n",
        "    else:\n",
        "        validation_file = \"../../data/QQA23_TaskB_qrcd_v1.2_dev_preprocessed.jsonl\"\n",
        "        # competition test phase\n",
        "        !python \"runners/run_qa.py\" \\\n",
        "            --model_name_or_path  $model_name \\\n",
        "            --dataset \"data_scripts/loader_scripts/qrcd_v1_2_dataset_loader.py\" \\\n",
        "            --do_train \\\n",
        "            --do_predict \\\n",
        "            --per_device_train_batch_size $batch_size \\\n",
        "            --learning_rate $lr \\\n",
        "            --num_train_epochs 15 \\\n",
        "            --max_seq_length 384 \\\n",
        "            --doc_stride 128 \\\n",
        "            --max_answer_length 35 \\\n",
        "            --output_dir $output_folder \\\n",
        "            --overwrite_output_dir  \\\n",
        "            --overwrite_cache \\\n",
        "            --train_file $train_file \\\n",
        "            --test_file  $validation_file \\\n",
        "            --save_total_limit 2 \\\n",
        "            --save_strategy \"epoch\" \\\n",
        "            --eval_metric \"metrics/QQA23_metric.py\" \\\n",
        "            --disable_early_stopping True \\\n",
        "            --pairwise_decoder $pairwise_decoder \\\n",
        "            --loss_type  $loss_type\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "PsNLwCpQssir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis and ensemble\n",
        "\n",
        "**Once the training is made you will find a dump file saved!**\n",
        "\n",
        "Something like: bert-base-arabertv02-fine-tuned-2e-05-first-827-QQA23_TaskB_qrcd_v1.2_train.zip\n",
        "This is a bert-base-arabertv02 fine-tuned model with:\n",
        "1. learning rate of 2e-05.\n",
        "2. first learning method.\n",
        "3. A random starting seed of 827.\n",
        "4. QQA23_TaskB_qrcd_v1.2_train training data is used\n",
        "\n",
        "This dump file contains all summary results and model predictions for each sample.\n",
        "You can look at the **analysis** directory of the repo for more details.\n",
        "You can group dump files into folders:\n",
        "1. run **performance_analysis.py** script\n",
        "2. Then run **ensemble_analysis.py** to get ensemble results from the **performance_analysis.py** script results.\n",
        "3. run **generate_reports.py**  to get excel aggregated results and kernel density plots for different runs.\n"
      ],
      "metadata": {
        "id": "WXI1IZUJ-SKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python analysis/performance_analysis.py"
      ],
      "metadata": {
        "id": "wwXLK2Kx-Rv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python analysis/ensemble_analysis.py"
      ],
      "metadata": {
        "id": "OoLey01k-jB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python analysis/generate_reports.py"
      ],
      "metadata": {
        "id": "vJa_QE2P-hF9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}