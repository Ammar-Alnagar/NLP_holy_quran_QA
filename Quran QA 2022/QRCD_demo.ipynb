{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDxbUn4SqkZn",
    "outputId": "4e0fa6ef-59dc-440c-9ca4-d7042f547a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 26 18:18:41 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.86                 Driver Version: 551.86         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   58C    P0             27W /   78W |     593MiB /   6144MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      6300    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      7668    C+G   D:\\Microsoft VS Code\\Code.exe               N/A      |\n",
      "|    0   N/A  N/A      8104    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      9660    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9684    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11336    C+G   ...8.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     11636    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A     12016    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     12276    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     15236    C+G   ...pcy8vm99wrpcg\\ModernFlyoutsHost.exe      N/A      |\n",
      "|    0   N/A  N/A     15684    C+G   ...64__v826wp6bftszj\\TranslucentTB.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2k2ZNV3dVPz",
    "outputId": "eb9c514d-e845-4b11-b1b1-04f3d178bf2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mrc-qrcd'...\n",
      "remote: Enumerating objects: 317, done.\u001b[K\n",
      "remote: Counting objects: 100% (317/317), done.\u001b[K\n",
      "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
      "remote: Total 317 (delta 127), reused 317 (delta 127), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (317/317), 45.96 MiB | 27.16 MiB/s, done.\n",
      "Resolving deltas: 100% (127/127), done.\n"
     ]
    }
   ],
   "source": [
    "repo_url = f\"https://github.com/mohammed-elkomy/quran-qa\"\n",
    "!git clone $repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqlqPLD7dXvQ",
    "outputId": "d22a95c4-7954-492a-aea9-f9a39eecd405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'mrc-qrcd'\n",
      "d:\\UNI\\Natural language processing\\project\\NLP_holy_quran_QA\\Quran QA 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "C:\\Users\\BOSS\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "!free -g\n",
    "%cd mrc-qrcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxZmUXEZRGs2",
    "outputId": "bcb83813-ae6b-4ac9-9741-3afca76e3802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\python\\lib\\site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in d:\\python\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\python\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\python\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\python\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\python\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in d:\\python\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in d:\\python\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\python\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\python\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\python\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\python\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in d:\\python\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in d:\\python\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in d:\\python\\lib\\site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in d:\\python\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in d:\\python\\lib\\site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\python\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\python\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\python\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\python\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\python\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farasapy==0.0.14 in d:\\python\\lib\\site-packages (0.0.14)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from farasapy==0.0.14) (2.31.0)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from farasapy==0.0.14) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests->farasapy==0.0.14) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->farasapy==0.0.14) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->farasapy==0.0.14) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->farasapy==0.0.14) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->farasapy==0.0.14) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in d:\\python\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in d:\\python\\lib\\site-packages (from sentence-transformers) (4.40.0)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\python\\lib\\site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: numpy in d:\\python\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\python\\lib\\site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in d:\\python\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in d:\\python\\lib\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in d:\\python\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\python\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\python\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\python\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\boss\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\python\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\python\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\python\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\python\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install farasapy==0.0.14\n",
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRhiMJqqUfC-"
   },
   "source": [
    "# qrcd\n",
    "\n",
    "to reproduce results you need to set seeds \n",
    "for example --seed 8045 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw-nfEryi82n"
   },
   "source": [
    "###  Arabert Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZSMXUcFIqaU"
   },
   "source": [
    "#### Eval phase\n",
    "training on qrcd/qrcd_v1.1_train.jsonl only\n",
    "\n",
    "seed values to use here\n",
    "```8045, 32558, 79727 ,30429 ,48910 ,46840 ,24384 ,55067 ,13718 ,16213 ,63304 ,40732 ,38609 ,22228 ,71549``` \n",
    "for exampel for 8045 \n",
    "you are supposed to download those files from colab into your local machine\n",
    "1. ```bert-large-arabertv02_1-eval-8045.dump```\n",
    "2. ```bert-large-arabertv02_1-8045.json```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4P_wdQHLj1ia",
    "outputId": "5f0dc69b-f7ad-4523-b025-c9eb848410b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\transformers\\utils\\import_utils.py:521: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "d:\\python\\Lib\\site-packages\\datasets\\load.py:929: FutureWarning: The repository for qrcd_dataset_loader contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at data/qrcd_dataset_loader.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\UNI\\Natural language processing\\project\\NLP_holy_quran_QA\\Quran QA 2022\\run_qa.py\", line 737, in <module>\n",
      "    main()\n",
      "  File \"d:\\UNI\\Natural language processing\\project\\NLP_holy_quran_QA\\Quran QA 2022\\run_qa.py\", line 300, in main\n",
      "    config = AutoConfig.from_pretrained(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\python\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 928, in from_pretrained\n",
      "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\python\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 631, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\python\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 686, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"d:\\python\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 369, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: aubmindlab/araelectra-base-discriminator does not appear to have a file named config.json. Checkout 'https://huggingface.co/aubmindlab/araelectra-base-discriminator/tree/main' for available files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_level:30\n",
      "04/26/2024 18:20:01 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "# !git pull\n",
    "# !rm -r \"bert-large-arabertv02_1\"\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path \"aubmindlab/araelectra-base-discriminator\" \\\n",
    "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 65 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 35 \\\n",
    "  --output_dir \"aubmindlab/araelectra-base-discriminator_1\" \\\n",
    "  --overwrite_output_dir  \\\n",
    "  --overwrite_cache \\\n",
    "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
    "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
    "  --save_total_limit 2 \\\n",
    "  --save_strategy \"epoch\" \\\n",
    "  --evaluation_strategy \"epoch\" \\\n",
    "  --load_best_model_at_end  True \\\n",
    "  --metric_for_best_model 'pRR' \\\n",
    "  --greater_is_better True \\\n",
    "  --eval_metric \"./data/qrcd_metric.py\" --fp16 \\\n",
    "  --seed 8045 \n",
    "\n",
    "  # per_device_train_batch_size is 6 for T4\n",
    "  # per_device_train_batch_size is 8 for P100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkkkBmrZI6hm"
   },
   "source": [
    "#### Test phase\n",
    " training on qrcd_v1.1_train_dev.json (both train + eval)\n",
    "\n",
    " seed values to use here\n",
    "```1114 ,18695 ,23293 ,27892 ,5748 ,59131 ,63847 ,68498 ,73133 ,77793 ,82431 ,87062 ,91701 ,94452 ,96475 ,98797``` \n",
    "for exampel for 8045 \n",
    "you are supposed to download those files from colab into your local machine\n",
    "1. ```bert-large-arabertv02_1-predict-1114.dump```\n",
    "2. ```bert-large-arabertv02_1-1114.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWQOYO9hnK26",
    "outputId": "b8562287-e516-41a1-fc10-6628cd826824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "log_level:20\n",
      "04/14/2022 08:34:02 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "04/14/2022 08:34:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=bert-large-arabertv02_1/runs/Apr14_08-34-02_8d20cc1b724d,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=40.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=bert-large-arabertv02_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=bert-large-arabertv02_1,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=1009,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/14/2022 08:34:02 - WARNING - datasets.builder - Using custom data configuration default-b2ae1bea58628d69\n",
      "04/14/2022 08:34:02 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/14/2022 08:34:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
      "04/14/2022 08:34:02 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
      "04/14/2022 08:34:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
      "100% 2/2 [00:00<00:00, 542.71it/s]\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:34:02,789 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:34:02,790 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "my configs BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.15,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.15,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:34:03,342 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:34:03,342 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/60e9ff28b93753c0d84eccffc9145d55e48e8993244148a01b72b99bb56acc31.292c9b563974181697c28c4ae4b6899dcaa7bdcf146b5682a389ef18208389a9\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/74b03364456a6ff9dc467962b740fe40d67a698eaf0ed8c0881fd836b1dec417.cf8d1bcc109a4ae89b00382b3c7dce421b2534c25751be3d5156322fc427fc12\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/13d0aeb6e3cef00c6474b9ff5fe5a23339fab57ac6cdf1640f741db78d01205a.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/a79ba04e38002933445acfd6e54a678537fe0d47b4c7ab8cb84e588b6adfb54d.b12f432f0d7c968f828692e21953045e83b24740d6985763c1f266215d87939a\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:34:05,266 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:34:05,267 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1772] 2022-04-14 08:34:05,622 >> loading weights file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5b6b65a52fdf663576d0fb750fd5af9041fbed101169d5b426f82f77cd80ab76.239c28acfb49bdb306b7935493251dfd48e2676d65a60030d1ad1b86bc2868e5\n",
      "[WARNING|modeling_utils.py:2049] 2022-04-14 08:34:10,659 >> Some weights of the model checkpoint at aubmindlab/bert-large-arabertv02 were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2060] 2022-04-14 08:34:10,659 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-large-arabertv02 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-e67ab3132c3b79b6.arrow\n",
      "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.13ba/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-59d4199539915e41.arrow\n",
      "Running tokenizer on prediction dataset: 100% 1/1 [00:01<00:00,  1.38s/ba]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-04-14 08:34:13,150 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[INFO|trainer.py:453] 2022-04-14 08:34:22,576 >> Using amp half precision backend\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1290] 2022-04-14 08:34:22,591 >> ***** Running training *****\n",
      "[INFO|trainer.py:1291] 2022-04-14 08:34:22,591 >>   Num examples = 821\n",
      "[INFO|trainer.py:1292] 2022-04-14 08:34:22,591 >>   Num Epochs = 40\n",
      "[INFO|trainer.py:1293] 2022-04-14 08:34:22,591 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1294] 2022-04-14 08:34:22,591 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1295] 2022-04-14 08:34:22,591 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1296] 2022-04-14 08:34:22,591 >>   Total optimization steps = 4120\n",
      "  0% 6/4120 [00:03<43:33,  1.57it/s]Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 737, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 686, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1422, in train\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2011, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2043, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1839, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1006, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 592, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 477, in forward\n",
      "    past_key_value=self_attn_past_key_value,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 409, in forward\n",
      "    output_attentions,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 334, in forward\n",
      "    attention_probs = self.dropout(attention_probs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1169, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 14.76 GiB total capacity; 13.21 GiB already allocated; 69.75 MiB free; 13.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0% 6/4120 [00:04<46:55,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "!rm -r \"bert-large-arabertv02_1\"\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path \"aubmindlab/bert-large-arabertv02\" \\\n",
    "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
    "  --do_train \\\n",
    "  --do_predict \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 40 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 35 \\\n",
    "  --output_dir \"bert-large-arabertv02_1\" \\\n",
    "  --overwrite_output_dir  \\\n",
    "  --overwrite_cache \\\n",
    "  --train_file qrcd/qrcd_v1.1_train_dev.jsonl \\\n",
    "  --test_file qrcd/qrcd_v1.1_test_noAnswers.jsonl \\\n",
    "  --save_strategy \"no\" \\\n",
    "  --eval_metric \"./data/qrcd_metric.py\" --fp16 \\\n",
    "  --seed 1009  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okMfegc-jR5-"
   },
   "source": [
    "### Arabert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE_WSldwLJHD"
   },
   "source": [
    "#### Eval phase\n",
    "training on qrcd/qrcd_v1.1_train.jsonl only\n",
    "\n",
    "seed values to use here\n",
    "```71338 ,67981 ,29808 ,67961 ,25668 ,20181 ,20178 ,67985 ,67982 ,23415 ,20172 ,20166 ,25982 ,27073 ,26612```\n",
    "for exampel for 71338 \n",
    "you are supposed to download those files from colab into your local machine\n",
    "1. ```bert-base-arabertv02-eval-71338.dump```\n",
    "2. ```bert-base-arabertv02-71338.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4vqTmjue5uC",
    "outputId": "37938faf-6234-43d1-af57-4b19a8d7ef11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_level:30\n",
      "04/26/2024 18:18:23 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\transformers\\utils\\import_utils.py:521: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "d:\\python\\Lib\\site-packages\\datasets\\load.py:929: FutureWarning: The repository for qrcd_dataset_loader contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at data/qrcd_dataset_loader.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\UNI\\Natural language processing\\project\\NLP_holy_quran_QA\\Quran QA 2022\\run_qa.py\", line 737, in <module>\n",
      "    main()\n",
      "  File \"d:\\UNI\\Natural language processing\\project\\NLP_holy_quran_QA\\Quran QA 2022\\run_qa.py\", line 300, in main\n",
      "    config = AutoConfig.from_pretrained(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\python\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 928, in from_pretrained\n",
      "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\python\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 631, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\python\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 686, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"d:\\python\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 369, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: aubmindlab/araelectra-base-discriminator does not appear to have a file named config.json. Checkout 'https://huggingface.co/aubmindlab/araelectra-base-discriminator/tree/main' for available files.\n"
     ]
    }
   ],
   "source": [
    "# !git pull\n",
    "# !rm -r \"bert-base-arabertv02\"\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path  \"aubmindlab/araelectra-base-discriminator\" \\\n",
    "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 50 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 35 \\\n",
    "  --output_dir \"aubmindlab/araelectra-base-discriminator\" \\\n",
    "  --overwrite_output_dir  \\\n",
    "  --overwrite_cache \\\n",
    "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
    "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
    "  --save_total_limit 2 \\\n",
    "  --save_strategy \"epoch\" \\\n",
    "  --evaluation_strategy \"epoch\" \\\n",
    "  --load_best_model_at_end  True \\\n",
    "  --metric_for_best_model 'pRR' \\\n",
    "  --greater_is_better True \\\n",
    "  --eval_metric \"./data/qrcd_metric.py\" \\\n",
    "  --seed 71338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJYXSBXeLK_x"
   },
   "source": [
    "#### Test phase\n",
    " training on qrcd_v1.1_train_dev.json (both train + eval)\n",
    "\n",
    " seed values to use here\n",
    "```54235 ,60998 ,64662 ,80936 ,80955 ,80959 ,80970 ,80988 ,82916 ,84448 ,84481 ,84665 ,84749 ,84871 ,87891 ,87917 ,88329 ,88469```\n",
    "for exampel for 54235 \n",
    "you are supposed to download those files from colab into your local machine\n",
    "1. ```bert-large-arabertv02_1-predict-54235.dump```\n",
    "2. ```bert-base-arabertv02-54235.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzurAveHjTcW",
    "outputId": "87200529-343d-49b2-8ad5-ca49b3cfde22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "log_level:20\n",
      "04/14/2022 08:35:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/14/2022 08:35:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=bert-base-arabertv02/runs/Apr14_08-35-36_8d20cc1b724d,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=32.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=bert-base-arabertv02,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=bert-base-arabertv02,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=54235,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/14/2022 08:35:36 - WARNING - datasets.builder - Using custom data configuration default-b2ae1bea58628d69\n",
      "04/14/2022 08:35:36 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/14/2022 08:35:36 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
      "04/14/2022 08:35:36 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
      "04/14/2022 08:35:36 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
      "100% 2/2 [00:00<00:00, 683.72it/s]\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:35:37,256 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:35:37,257 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "my configs BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.15,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.15,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:35:37,802 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:35:37,803 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,445 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6d195dfa0ab2ff9e30278a35699eb13eeb3a69731e8458ef8af7c9598e05ee99.292c9b563974181697c28c4ae4b6899dcaa7bdcf146b5682a389ef18208389a9\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,445 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/cdf2434ef735735269b56feeae539fb93d28d674f4b335e2f08ed38960ea51e7.cf8d1bcc109a4ae89b00382b3c7dce421b2534c25751be3d5156322fc427fc12\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,446 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,446 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/4318ddb7acd24ce55328cd47235c5aaf88d4925a8a257863c4ebcc0852985d2c.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,446 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f8b57973211cf183180900e4646e204fe0c492b6129df1509f0e8ddc02369a05.130b8595f3c840efdfe3e2e9d3926b3c47a1a69aa1a7f6660fd11be56bf0d5fc\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:35:39,717 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:35:39,718 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1772] 2022-04-14 08:35:40,080 >> loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c0183275aaa055648fb44d6c24d9604b860b02398c7d9c7bea64257cdb87a56e.9e74f71b2acf9ee9289bd6ab56938934df2792f595150523e3f83558666a9676\n",
      "[WARNING|modeling_utils.py:2049] 2022-04-14 08:35:41,821 >> Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2060] 2022-04-14 08:35:41,821 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:35:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-1ca3c1843a3c0793.arrow\n",
      "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  2.53ba/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:35:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-c09dd0daf5d91111.arrow\n",
      "Running tokenizer on prediction dataset: 100% 1/1 [00:01<00:00,  1.19s/ba]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-04-14 08:35:43,643 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "!rm -r \"bert-base-arabertv02\"\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path  \"aubmindlab/bert-base-arabertv02\" \\\n",
    "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
    "  --do_train \\\n",
    "  --do_predict \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 32 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 35 \\\n",
    "  --output_dir \"bert-base-arabertv02\" \\\n",
    "  --overwrite_output_dir  \\\n",
    "  --overwrite_cache \\\n",
    "  --train_file qrcd/qrcd_v1.1_train_dev.jsonl \\\n",
    "  --test_file qrcd/qrcd_v1.1_test_noAnswers.jsonl \\\n",
    "  --save_strategy \"no\" \\\n",
    "  --eval_metric \"./data/qrcd_metric.py\" \\\n",
    "  --seed 54235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxJoExNti-Pe"
   },
   "source": [
    "### ARBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd39qyCuLQ-A"
   },
   "source": [
    "#### Eval phase\n",
    "training on qrcd/qrcd_v1.1_train.jsonl only\n",
    "\n",
    "seed values to use here\n",
    "```64976 ,64988 ,73862 ,84804 ,79583 ,81181 ,59377 ,59382 ,73869 ,77564 ,79723 ,64952 ,73865 ,59373 ,84349``` \n",
    "for exampel for 64976 \n",
    "you are supposed to download those files from colab into your local machine\n",
    "1. ```ARBERT-eval-64976.dump```\n",
    "2. ```ARBERT-64976.json```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kp5B-bcDjFPe",
    "outputId": "79a5db0f-a4cd-4053-ca3d-68585894ded9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "log_level:20\n",
      "04/14/2022 08:35:54 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/14/2022 08:35:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=ARBERT/runs/Apr14_08-35-54_8d20cc1b724d,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=pRR,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=50.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=ARBERT,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=ARBERT,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=2,\n",
      "seed=64976,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/14/2022 08:35:54 - WARNING - datasets.builder - Using custom data configuration default-cd7047838bfb5afc\n",
      "04/14/2022 08:35:54 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/14/2022 08:35:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
      "04/14/2022 08:35:54 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
      "04/14/2022 08:35:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
      "100% 2/2 [00:00<00:00, 637.92it/s]\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:35:55,132 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:35:55,132 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "my configs BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.15,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.15,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:35:55,701 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:35:55,701 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,352 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b666228ad7d67661ab245a67d10a34dfa93d9fd5de5ef3c6a607a92a9db04766.48871a2b1d848c60db4344a74b461797d9913433a790a90861ff76b4e847d1bc\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,352 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,352 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,352 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/110b3bd85e0675de74f72d1ba2e29b31cd09cd69aa8f70d9037b706f3bcc26fb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,353 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/41d233d78ec59e1817ce96e1283631b3579e030eb979d766e25eaf3e1f0362ff.e2391f37093ecddb817287261d0566d2f027887d636f1764ee422c1977ae6ac1\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:35:57,630 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:35:57,631 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:35:58,027 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:35:58,028 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1772] 2022-04-14 08:35:58,377 >> loading weights file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f37a41178e5c357491b1adb9fe079d6e41ff54338328ea61fbd2a155b6c782ea.ef8eca7a93e9680c24baaba838665035caca0660ec8613de69adea7f19b57817\n",
      "[WARNING|modeling_utils.py:2049] 2022-04-14 08:36:02,603 >> Some weights of the model checkpoint at UBC-NLP/ARBERT were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2060] 2022-04-14 08:36:02,603 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at UBC-NLP/ARBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:36:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-4f1d2412b8589d81.arrow\n",
      "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  2.20ba/s]\n",
      "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:36:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-1873f793a57502c8.arrow\n",
      "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  2.02ba/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-04-14 08:36:03,757 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 737, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 657, in main\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 1395, in load_metric\n",
      "    metric_cls = import_main_class(metric_module, dataset=False)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 106, in import_main_class\n",
      "    module = importlib.import_module(module_path)\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/qrcd_metric/c7f476a4a9147242a1c8b46c61cfd7505b59aa7be38faecbe820cf17f59c9dcf/qrcd_metric.py\", line 142, in <module>\n",
      "    from .qrcd_eval import evaluate\n",
      "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/qrcd_metric/c7f476a4a9147242a1c8b46c61cfd7505b59aa7be38faecbe820cf17f59c9dcf/qrcd_eval.py\", line 18, in <module>\n",
      "    farasa_segmenter = FarasaSegmenter(interactive=True)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 54, in __init__\n",
      "    self._initialize_task()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 140, in _initialize_task\n",
      "    return self._run_task_interactive(bword)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 215, in _run_task_interactive\n",
      "    output = self.__task_proc.stdout.readline().decode(\"utf8\").strip()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "!rm -r \"ARBERT\"\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path  \"UBC-NLP/ARBERT\" \\\n",
    "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 50 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 35 \\\n",
    "  --output_dir \"ARBERT\" \\\n",
    "  --overwrite_output_dir  \\\n",
    "  --overwrite_cache \\\n",
    "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
    "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
    "  --save_total_limit 2 \\\n",
    "  --save_strategy \"epoch\" \\\n",
    "  --evaluation_strategy \"epoch\" \\\n",
    "  --load_best_model_at_end  True \\\n",
    "  --metric_for_best_model 'pRR' \\\n",
    "  --greater_is_better True \\\n",
    "  --eval_metric \"./data/qrcd_metric.py\" \\\n",
    "  --seed 64976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0RJ0doSLPYR"
   },
   "source": [
    "#### Test phase\n",
    " training on qrcd_v1.1_train_dev.json (both train + eval)\n",
    "\n",
    " seed values to use here\n",
    "```107 ,14 ,43919 ,47360 ,50798 ,57621 ,86829 ,88813 ,90781 ,91496 ,91533 ,94949 ,95000 ,96521 ,96552 ,98412 ,98465``` \n",
    "for exampel for 107 \n",
    "you are supposed to download those files from colab into your local machine\n",
    "1. ```ARBERT-predict-107.dump```\n",
    "2. ```ARBERT-107.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0idNmCIlOEoB",
    "outputId": "4c13b0a5-6cca-400d-d024-f61c2ce14fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "log_level:20\n",
      "04/14/2022 08:36:13 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/14/2022 08:36:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=ARBERT/runs/Apr14_08-36-13_8d20cc1b724d,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=32.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=ARBERT,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=ARBERT,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=107,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/14/2022 08:36:14 - WARNING - datasets.builder - Using custom data configuration default-b2ae1bea58628d69\n",
      "04/14/2022 08:36:14 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/14/2022 08:36:14 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
      "04/14/2022 08:36:14 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
      "04/14/2022 08:36:14 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
      "100% 2/2 [00:00<00:00, 448.83it/s]\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:36:14,465 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:36:14,466 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "my configs BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.15,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.15,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:36:15,019 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:36:15,020 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b666228ad7d67661ab245a67d10a34dfa93d9fd5de5ef3c6a607a92a9db04766.48871a2b1d848c60db4344a74b461797d9913433a790a90861ff76b4e847d1bc\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/110b3bd85e0675de74f72d1ba2e29b31cd09cd69aa8f70d9037b706f3bcc26fb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/41d233d78ec59e1817ce96e1283631b3579e030eb979d766e25eaf3e1f0362ff.e2391f37093ecddb817287261d0566d2f027887d636f1764ee422c1977ae6ac1\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:36:16,949 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:36:16,950 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-04-14 08:36:17,339 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
      "[INFO|configuration_utils.py:690] 2022-04-14 08:36:17,340 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1772] 2022-04-14 08:36:17,707 >> loading weights file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f37a41178e5c357491b1adb9fe079d6e41ff54338328ea61fbd2a155b6c782ea.ef8eca7a93e9680c24baaba838665035caca0660ec8613de69adea7f19b57817\n",
      "[WARNING|modeling_utils.py:2049] 2022-04-14 08:36:19,680 >> Some weights of the model checkpoint at UBC-NLP/ARBERT were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2060] 2022-04-14 08:36:19,681 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at UBC-NLP/ARBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:36:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-db7a282f2bb70a9d.arrow\n",
      "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.99ba/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:36:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-6b874c14a49a95d2.arrow\n",
      "Running tokenizer on prediction dataset: 100% 1/1 [00:01<00:00,  1.21s/ba]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-04-14 08:36:21,579 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 737, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 657, in main\n",
      "    metric = load_metric(metric_file)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 1395, in load_metric\n",
      "    metric_cls = import_main_class(metric_module, dataset=False)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 106, in import_main_class\n",
      "    module = importlib.import_module(module_path)\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/qrcd_metric/c7f476a4a9147242a1c8b46c61cfd7505b59aa7be38faecbe820cf17f59c9dcf/qrcd_metric.py\", line 142, in <module>\n",
      "    from .qrcd_eval import evaluate\n",
      "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/qrcd_metric/c7f476a4a9147242a1c8b46c61cfd7505b59aa7be38faecbe820cf17f59c9dcf/qrcd_eval.py\", line 18, in <module>\n",
      "    farasa_segmenter = FarasaSegmenter(interactive=True)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 54, in __init__\n",
      "    self._initialize_task()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 140, in _initialize_task\n",
      "    return self._run_task_interactive(bword)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 215, in _run_task_interactive\n",
      "    output = self.__task_proc.stdout.readline().decode(\"utf8\").strip()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "!rm -r \"ARBERT\"\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path  \"UBC-NLP/ARBERT\" \\\n",
    "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
    "  --do_train \\\n",
    "  --do_predict \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 32 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 35 \\\n",
    "  --output_dir \"ARBERT\" \\\n",
    "  --overwrite_output_dir  \\\n",
    "  --overwrite_cache \\\n",
    "  --train_file qrcd/qrcd_v1.1_train_dev.jsonl \\\n",
    "  --test_file qrcd/qrcd_v1.1_test_noAnswers.jsonl \\\n",
    "  --save_strategy \"no\" \\\n",
    "  --eval_metric \"./data/qrcd_metric.py\" \\\n",
    "  --seed 107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HN9JYTbjJ7l"
   },
   "source": [
    "### Other runs not included in the reported scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTO9ssIMjNqO",
    "outputId": "38553cb6-498e-4c9f-e986-ca408227c90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "rm: cannot remove 'qarib1': No such file or directory\n",
      "log_level:20\n",
      "03/15/2022 12:02:16 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/15/2022 12:02:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=qarib1/runs/Mar15_12-02-16_341417b62003,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=pRR,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=40.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=qarib1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=qarib1,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=5,\n",
      "seed=45736,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/15/2022 12:02:16 - WARNING - datasets.builder - Using custom data configuration default-12fd5740a033cd66\n",
      "03/15/2022 12:02:16 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/15/2022 12:02:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
      "03/15/2022 12:02:16 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8)\n",
      "03/15/2022 12:02:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
      "100% 3/3 [00:00<00:00, 513.94it/s]\n",
      "[INFO|file_utils.py:2215] 2022-03-15 12:02:17,097 >> https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpx7pblms7\n",
      "Downloading: 100% 576/576 [00:00<00:00, 688kB/s]\n",
      "[INFO|file_utils.py:2219] 2022-03-15 12:02:17,235 >> storing https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|file_utils.py:2227] 2022-03-15 12:02:17,235 >> creating metadata file for /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:02:17,236 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:02:17,237 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "my configs BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.15,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.15,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 712, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 314, in main\n",
      "    use_auth_token=True if model_args.use_auth_token else None,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\", line 471, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\", line 341, in get_tokenizer_config\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 2317, in get_file_from_repo\n",
      "    use_auth_token=use_auth_token,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1929, in cached_path\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 2124, in get_from_cache\n",
      "    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/api.py\", line 104, in head\n",
      "    return request('head', url, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 677, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\", line 160, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/util/connection.py\", line 74, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt\n",
      "Already up to date.\n",
      "rm: cannot remove 'qarib2': No such file or directory\n",
      "log_level:20\n",
      "03/15/2022 12:02:23 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/15/2022 12:02:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=qarib2/runs/Mar15_12-02-23_341417b62003,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=pRR,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=40.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=qarib2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=qarib2,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=5,\n",
      "seed=45743,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/15/2022 12:02:23 - WARNING - datasets.builder - Using custom data configuration default-12fd5740a033cd66\n",
      "03/15/2022 12:02:23 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/15/2022 12:02:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
      "03/15/2022 12:02:23 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8)\n",
      "03/15/2022 12:02:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
      "100% 3/3 [00:00<00:00, 542.27it/s]\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:02:23,811 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:02:23,812 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "my configs BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.15,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.15,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2022-03-15 12:02:23,945 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:02:24,084 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:02:24,084 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:2215] 2022-03-15 12:02:24,366 >> https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0q8j1rlk\n",
      "Downloading: 100% 701k/701k [00:00<00:00, 4.76MB/s]\n",
      "[INFO|file_utils.py:2219] 2022-03-15 12:02:24,656 >> storing https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
      "[INFO|file_utils.py:2227] 2022-03-15 12:02:24,656 >> creating metadata file for /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,239 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,240 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,240 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,240 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,240 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:02:25,380 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:02:25,381 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:02:25,592 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:02:25,593 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:2215] 2022-03-15 12:02:25,804 >> https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbhdda4d3\n",
      "Downloading: 100% 518M/518M [00:25<00:00, 21.1MB/s]\n",
      "[INFO|file_utils.py:2219] 2022-03-15 12:02:51,843 >> storing https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
      "[INFO|file_utils.py:2227] 2022-03-15 12:02:51,843 >> creating metadata file for /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
      "[INFO|modeling_utils.py:1431] 2022-03-15 12:02:51,844 >> loading weights file https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
      "[WARNING|modeling_utils.py:1694] 2022-03-15 12:02:54,073 >> Some weights of the model checkpoint at qarib/bert-base-qarib were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1705] 2022-03-15 12:02:54,073 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at qarib/bert-base-qarib and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:02:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-cf6826cad1a41ea8.arrow\n",
      "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.82ba/s]\n",
      "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:02:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-8224c2ca68ab3036.arrow\n",
      "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  1.67ba/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:02:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-57a98f5b05932d61.arrow\n",
      "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  1.34ba/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-03-15 12:02:56,208 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1279] 2022-03-15 12:03:03,987 >> ***** Running training *****\n",
      "[INFO|trainer.py:1280] 2022-03-15 12:03:03,987 >>   Num examples = 712\n",
      "[INFO|trainer.py:1281] 2022-03-15 12:03:03,987 >>   Num Epochs = 40\n",
      "[INFO|trainer.py:1282] 2022-03-15 12:03:03,987 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1283] 2022-03-15 12:03:03,987 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1284] 2022-03-15 12:03:03,988 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1285] 2022-03-15 12:03:03,988 >>   Total optimization steps = 1800\n",
      "{'loss': 1.806, 'learning_rate': 1.4444444444444446e-05, 'epoch': 11.11}\n",
      "{'loss': 0.4586, 'learning_rate': 8.888888888888888e-06, 'epoch': 22.22}\n",
      " 64% 1160/1800 [50:54<28:21,  2.66s/it]Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 712, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 659, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1400, in train\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2002, in training_step\n",
      "    loss.backward()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n",
      "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      " 64% 1160/1800 [50:56<28:06,  2.64s/it]\n",
      "Already up to date.\n",
      "rm: cannot remove 'qarib3': No such file or directory\n",
      "log_level:20\n",
      "03/15/2022 12:54:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/15/2022 12:54:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=qarib3/runs/Mar15_12-54-06_341417b62003,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=pRR,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=40.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=qarib3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=qarib3,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=5,\n",
      "seed=48846,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/15/2022 12:54:06 - WARNING - datasets.builder - Using custom data configuration default-12fd5740a033cd66\n",
      "03/15/2022 12:54:06 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/15/2022 12:54:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
      "03/15/2022 12:54:06 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8)\n",
      "03/15/2022 12:54:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
      "100% 3/3 [00:00<00:00, 663.45it/s]\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:54:07,134 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:54:07,135 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "my configs BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.15,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.15,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2022-03-15 12:54:07,281 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:54:07,410 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:54:07,411 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,215 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,215 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,215 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,216 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,216 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:54:08,352 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:54:08,353 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:54:08,554 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:54:08,554 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1431] 2022-03-15 12:54:08,751 >> loading weights file https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 712, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 329, in main\n",
      "    use_auth_token=True if model_args.use_auth_token else None,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\", line 447, in from_pretrained\n",
      "    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 1493, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1791, in __init__\n",
      "    self.bert = BertModel(config, add_pooling_layer=False)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 866, in __init__\n",
      "    self.encoder = BertEncoder(config)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 534, in __init__\n",
      "    self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 534, in <listcomp>\n",
      "    self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 450, in __init__\n",
      "    self.attention = BertAttention(config)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 370, in __init__\n",
      "    self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 241, in __init__\n",
      "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 90, in __init__\n",
      "    self.reset_parameters()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 96, in reset_parameters\n",
      "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\", line 395, in kaiming_uniform_\n",
      "    return tensor.uniform_(-bound, bound)\n",
      "KeyboardInterrupt\n",
      "Already up to date.\n",
      "rm: cannot remove 'qarib4': No such file or directory\n",
      "log_level:20\n",
      "03/15/2022 12:54:16 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/15/2022 12:54:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=qarib4/runs/Mar15_12-54-16_341417b62003,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=pRR,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=40.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=qarib4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=qarib4,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=5,\n",
      "seed=48856,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/15/2022 12:54:16 - WARNING - datasets.builder - Using custom data configuration default-12fd5740a033cd66\n",
      "03/15/2022 12:54:16 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/15/2022 12:54:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
      "03/15/2022 12:54:16 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8)\n",
      "03/15/2022 12:54:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
      "100% 3/3 [00:00<00:00, 616.30it/s]\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:54:16,616 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:54:16,617 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "my configs BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.15,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.15,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2022-03-15 12:54:16,785 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:54:16,923 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:54:16,923 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:54:17,895 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:54:17,896 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:648] 2022-03-15 12:54:18,107 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
      "[INFO|configuration_utils.py:684] 2022-03-15 12:54:18,108 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1431] 2022-03-15 12:54:18,303 >> loading weights file https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
      "[WARNING|modeling_utils.py:1694] 2022-03-15 12:54:20,454 >> Some weights of the model checkpoint at qarib/bert-base-qarib were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1705] 2022-03-15 12:54:20,454 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at qarib/bert-base-qarib and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:54:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-56caba4c5eb7d701.arrow\n",
      "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.94ba/s]\n",
      "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:54:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-8224c2ca68ab3036.arrow\n",
      "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  1.81ba/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:54:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-57a98f5b05932d61.arrow\n",
      "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  1.37ba/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-03-15 12:54:22,476 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1279] 2022-03-15 12:54:30,037 >> ***** Running training *****\n",
      "[INFO|trainer.py:1280] 2022-03-15 12:54:30,037 >>   Num examples = 712\n",
      "[INFO|trainer.py:1281] 2022-03-15 12:54:30,037 >>   Num Epochs = 40\n",
      "[INFO|trainer.py:1282] 2022-03-15 12:54:30,037 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1283] 2022-03-15 12:54:30,037 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1284] 2022-03-15 12:54:30,037 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1285] 2022-03-15 12:54:30,037 >>   Total optimization steps = 1800\n",
      "  0% 1/1800 [00:02<1:19:24,  2.65s/it]"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "!rm -r \"qarib\"\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path \"qarib/bert-base-qarib\" \\\n",
    "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_predict \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 50 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 35 \\\n",
    "  --output_dir \"qarib\" \\\n",
    "  --overwrite_output_dir  \\\n",
    "  --overwrite_cache \\\n",
    "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
    "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
    "  --test_file qrcd/qrcd_v1.1_my_test.jsonl \\\n",
    "  --save_total_limit 2 \\\n",
    "  --save_strategy \"epoch\" \\\n",
    "  --evaluation_strategy \"epoch\" \\\n",
    "  --load_best_model_at_end  True \\\n",
    "  --metric_for_best_model 'pRR' \\\n",
    "  --greater_is_better True \\\n",
    "  --eval_metric \"./data/qrcd_metric.py\" \n",
    "\n",
    "!git pull\n",
    "!rm -r \"MARBERT\"\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path  \"UBC-NLP/MARBERT\" \\\n",
    "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_predict \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 50 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 35 \\\n",
    "  --output_dir \"MARBERT\" \\\n",
    "  --overwrite_output_dir  \\\n",
    "  --overwrite_cache \\\n",
    "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
    "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
    "  --test_file qrcd/qrcd_v1.1_my_test.jsonl \\\n",
    "  --save_total_limit 2 \\\n",
    "  --save_strategy \"epoch\" \\\n",
    "  --evaluation_strategy \"epoch\" \\\n",
    "  --load_best_model_at_end  True \\\n",
    "  --metric_for_best_model 'pRR' \\\n",
    "  --greater_is_better True \\\n",
    "  --eval_metric \"./data/qrcd_metric.py\" "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3HN9JYTbjJ7l"
   ],
   "name": "QRCD demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
